@inproceedings{Vaswani+2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3356--3369},
  year={2020}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{schwenk2022okvqa,
  title={A-OKVQA: A Benchmark for Visual Question Answering Using World Knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={European Conference on Computer Vision},
  pages={146--162},
  year={2022}
}

@inproceedings{shin2020autoprompt,
  title={AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4222--4235},
  year={2020}
}

@inproceedings{nie2022diffusion,
  title={Diffusion Models for Adversarial Purification},
  author={Nie, Weili and Guo, Brandon and Huang, Yujia and Xiao, Chaowei and Vahdat, Arash and Anandkumar, Animashree},
  booktitle={ICML},
  pages={16805--16827},
  year={2022},
  organization={PMLR}
}

@inproceedings{robey2023smoothllm,
  title={SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks},
  author={Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George},
  booktitle={R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models},
  year={2023}
}



@inproceedings{fang2023eva,
  title={Eva: Exploring the limits of masked visual representation learning at scale},
  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  booktitle={CVPR},
  pages={19358--19369},
  year={2023}
}

@misc{openaiGPT4VisionSystem,
	author = {OpenAI},
	title = {{G}{P}{T}-4{V}(ision) system card --- openai.com},
	howpublished = {\url{https://openai.com/research/gpt-4v-system-card}},
	year = {2023},
	note = {[Accessed 19-03-2024]},
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}



@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv:2307.09288},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}


@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv:2304.10592},
  year={2023}
}


@article{dai2023instructblip,
  title        = {Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023},
  author       = {Dai, W and Li, J and Li, D and Tiong, AMH and Zhao, J and Wang, W and Li, B and Fung, P and Hoi, S},
  year         = 2023,
  journal      = {arXiv:2305.06500}
}

@inproceedings{chao2023jailbreaking,
  title={Jailbreaking Black Box Large Language Models in Twenty Queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  booktitle={R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models},
  year={2023}
}

@inproceedings{croce2019sparse,
  title={Sparse and imperceivable adversarial attacks},
  author={Croce, Francesco and Hein, Matthias},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4724--4732},
  year={2019}
}

@inproceedings{madry2018towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}

@inproceedings{chen2023semi,
	title        = {Semi-Offline Reinforcement Learning for Optimized Text Generation},
	author       = {Chen, Changyu and Wang, Xiting and Jin, Yiqiao and Dong, Victor Ye and Dong, Li and Cao, Jie and Liu, Yi and Yan, Rui},
	year         = 2023,
	booktitle    = {ICML}
}
@article{xiao2023large,
	title        = {Large Language Models Can Be Good Privacy Protection Learners},
	author       = {Xiao, Yijia and Jin, Yiqiao and Bai, Yushi and Wu, Yue and Yang, Xianjun and Luo, Xiao and Yu, Wenchao and Zhao, Xujiang and Liu, Yanchi and Chen, Haifeng and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2310.02469}
}
@article{jin2023better,
	title        = {Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries},
	author       = {Jin, Yiqiao and Chandra, Mohit and Verma, Gaurav and Hu, Yibo and De Choudhury, Munmun and Kumar, Srijan},
	year         = 2023,
	journal      = {arXiv e-prints},
	pages        = {arXiv--2310}
}

@article{awadalla2023openflamingo,
  title={Openflamingo: An open-source framework for training large autoregressive vision-language models},
  author={Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal={arXiv:2308.01390},
  year={2023}
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={NeurIPS},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{zhao2023competeai,
	title        = {Competeai: Understanding the competition behaviors in large language model-based agents},
	author       = {Zhao, Qinlin and Wang, Jindong and Zhang, Yixuan and Jin, Yiqiao and Zhu, Kaijie and Chen, Hao and Xie, Xing},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2310.17512}
}

@inproceedings{wang2023robustness,
  title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective},
  author={Wang, Jindong and Xixu, Hu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Ye, Wei and Huang, Haojun and Geng, Xiubo and others},
  booktitle={ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models},
  year={2023}
}



@misc{qi2023visual,
      title={Visual Adversarial Examples Jailbreak Aligned Large Language Models}, 
      author={Xiangyu Qi and Kaixuan Huang and Ashwinee Panda and Peter Henderson and Mengdi Wang and Prateek Mittal},
      year={2023},
      eprint={2306.13213},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{niu2024jailbreaking,
  title={Jailbreaking attack against multimodal large language model},
  author={Niu, Zhenxing and Ren, Haodong and Gao, Xinbo and Hua, Gang and Jin, Rong},
  journal={arXiv preprint arXiv:2402.02309},
  year={2024}
}

@inproceedings{kumar2023advances,
  title={Advances in AI for safety, equity, and well-being on web and social media: detection, robustness, attribution, and mitigation},
  author={Kumar, Srijan},
  booktitle={AAAI},
  volume={37},
  number={13},
  pages={15444--15444},
  year={2023}
}

@article{wang2023abspyramid,
  title={Abspyramid: Benchmarking the abstraction ability of language models with a unified entailment graph},
  author={Wang, Zhaowei and Shi, Haochen and Wang, Weiqi and Fang, Tianqing and Zhang, Hongming and Choi, Sehyun and Liu, Xin and Song, Yangqiu},
  journal={arXiv:2311.09174},
  year={2023}
}


@inproceedings{shayegani2023jailbreak,
  title={Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models},
  author={Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
  booktitle={ICLR},
  year={2023}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{xu2023sc,
  title={Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese},
  author={Xu, Liang and Zhao, Kangkang and Zhu, Lei and Xue, Hang},
  journal={arXiv:2310.05818},
  year={2023}
}

@article{yi2024survey,
  title={A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems},
  author={Yi, Zihao and Ouyang, Jiarui and Liu, Yuwen and Liao, Tianhao and Xu, Zhe and Shen, Ying},
  journal={arXiv:2402.18013},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv:2312.11805},
  year={2023}
}

@article{openai2023gpt4,
  title        = {GPT-4 Technical Report},
  author       = {OpenAI},
  year         = 2023,
  journal      = {Arxiv Preprint},
  volume       = {arXiv:2303.08774},
  url          = {https://arxiv.org/abs/2303.08774},
  eprint       = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}


@article{yu2023g,
  title={G2uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering},
  author={Yu, Hao and Ma, Chuan and Liu, Meng and Liu, Xinwang and Liu, Zhe and Ding, Ming},
  journal={arXiv:2306.04984},
  year={2023}
}

@inproceedings{zhang2023foundation,
  title={Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models},
  author={Zhang, Peiyan and Liu, Haoyang and Li, Chaozhuo and Xie, Xing and Kim, Sunghun and Wang, Haohan},
  booktitle={ICLR},
  year={2023}
}

@article{jin2024mm,
  title={MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms},
  author={Jin, Yiqiao and Choi, Minje and Verma, Gaurav and Wang, Jindong and Kumar, Srijan},
  journal={arXiv:2402.14154},
  year={2024}
}

@inproceedings{nookala2023adversarial,
  title={Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding},
  author={Nookala, Venkata Prabhakara Sarath and Verma, Gaurav and Mukherjee, Subhabrata and Kumar, Srijan},
  booktitle={ACL},
  year={2023}
}


@misc{bailey2023image,
  title={Image Hijacks: Adversarial Images can Control Generative Models at Runtime}, 
  author={Luke Bailey and Euan Ong and Stuart Russell and Scott Emmons},
  year={2023},
  eprint={2309.00236},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{liu2023improvedllava,
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      title={Improved Baselines with Visual Instruction Tuning}, 
      publisher={arXiv:2310.03744},
      year={2023},
}


@inproceedings{liu2023llava,
author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
title       = {Visual Instruction Tuning},
booktitle   = {NeurIPS},
year        = {2023}
}